{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Simple Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. An IR system returns 10 relevant documents and 8 non-relevant documents. There are a total of 25 relevant documents in the collection. What is the precision of the system on this search, and what is its recall?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = true positives / (true positives + false positives)\n",
    "\n",
    "Recall = true positives / (true positives + false negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 / (10 + 8) # precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 / (10 + 15) # recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Draw the inverted index that would be built for the following document collection.** \n",
    "\n",
    "Doc 1: new home sales top forecasts\n",
    "\n",
    "Doc 2: home sales rise in july\n",
    "\n",
    "Doc 3: increase in home sales in july\n",
    "\n",
    "Doc 4: july new home sales rise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple inverted index can be built by collection documents (above), tokenizing each document, preprocess to normalized tokens (indexing terms), and finally indexing the documents in an inverted index, consisting of a dict and postings. Lets just build it before we draw it. According to Manning's Information Retrieval, inverted index structures are essentially without rivals as the most efficient structure for supporting ad hoc text search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "docs = [\"new home sales, top forecasts#\", # inserted some non-alpa-numeric entities demonstrate normalization.\n",
    "        \"home sales rise, in #july\",\n",
    "        \"increase in home, sales in july\",\n",
    "        \"july new home#:)/--'' sales, rise\"] # list of docs\n",
    "\n",
    "tokenized_docs = [word_tokenize(s) for s in docs]\n",
    "normalized_docs = [[word.lower() for word in s if word.isalpha()] for s in tokenized_docs] # we also redudantly lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inv_indx = defaultdict(list) # using a defaultdict provides a defaul value for a nonexistent key as to avoid KeyErrors\n",
    "for idx, text in enumerate(normalized_docs): # enumerating over the list of normalized docs and their indexes\n",
    "    for word in text: \n",
    "        inv_indx[word].append(idx) # appending the indexes to which every word belongs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'new': [0, 3],\n",
       "             'home': [0, 1, 2, 3],\n",
       "             'sales': [0, 1, 2, 3],\n",
       "             'top': [0],\n",
       "             'forecasts': [0],\n",
       "             'rise': [1, 3],\n",
       "             'in': [1, 2, 2],\n",
       "             'july': [1, 2, 3],\n",
       "             'increase': [2]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_indx # I hope that the output below counts as a drawing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Consider two documents A and B whose Euclidean distance is d and cosine similarity is c (using no normalization other than raw term frequencies). If we create a new document A' by appending A to itself and another document B' by appending B to itself, then:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**a) What is the Euclidean distance between A' and B' (using raw term frequency)?**\n",
    "\n",
    "\n",
    "\n",
    "**Short answer:**\n",
    "The Euclidean Distance between A' and B' is $d^2$, e.g, for the example below, 3.1622776601683795 ** 2 = 6.324555320336759‬\n",
    "\n",
    "**Long answer:** \n",
    "See below until subquestion b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equaton for euclidian distance between 2 data objects is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d\\left( p,q\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( q_{i}-p_{i}\\right)^2 } $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where n is the number of dimensions / attributes, $p_k$ and $q_k$ are, respectively, the $Kth$ attributes of data objects p and q. Which basically means that we compute the distance of the individual attributes, sqaure it and then sum it all, before we take the square root of the sum. Let's run through an example, using raw term frequency (as opposed to TF-IDF maybe) and 2 sentences of equal length, len = 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"the quicker brown dogs easily jumps over the lazy dogs\" \n",
    "doc2 = \"the quicker dogs pose a serious problem for lazy dogs\"\n",
    "corpus = [doc1, doc2]\n",
    "len(doc1.split()), len(doc1.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we calculate raw the term frequencies for each doc, using the equation: tf(t,d) = count of t in d / number of words in d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 2, 'quicker': 1, 'brown': 1, 'dogs': 2, 'easily': 1, 'jumps': 1, 'over': 1, 'lazy': 1}\n"
     ]
    }
   ],
   "source": [
    "def calc_term_frequency(doc : list):\n",
    "    \n",
    "    dic = {}\n",
    "    for word in doc.split():\n",
    "        if word in dic:\n",
    "            dic[word] = dic[word] + 1\n",
    "        else:\n",
    "            dic[word]= 1\n",
    "    \n",
    "    for word, frequency in dic.items():\n",
    "       dic[word] = frequency\n",
    "    \n",
    "    return dic\n",
    "\n",
    "tfs_doc1 = calc_term_frequency(doc1)\n",
    "tfs_doc2 = calc_term_frequency(doc2)\n",
    "print(tfs_doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate their inter partes euclidian distance. We can use SciKit-learn to check that we got the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1622776601683795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(sum((tfs_doc1.get(k, 0) - tfs_doc1.get(k, 0))**2 for k in set(tfs_doc1.keys()).union(set(tfs_doc1.keys())))) # output: 0\n",
    "math.sqrt(sum((tfs_doc1.get(k, 0) - tfs_doc2.get(k, 0))**2 for k in set(tfs_doc1.keys()).union(set(tfs_doc2.keys())))) # output: 0.316227766016838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n",
      "[[3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "corpus_vect = CountVectorizer().fit_transform(corpus).todense() \n",
    "\n",
    "print(euclidean_distances(corpus_vect[0], corpus_vect[0])) # output: 0\n",
    "print(euclidean_distances(corpus_vect[0], corpus_vect[1] )) # output: 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computed distances above is the same as the one we got manually, when we round them down. Which means our base euclidian distance, d = 0.316227766016838\n",
    "\n",
    "Now, let's answer the question, what happens if we append each doc to itself and then run the whole thing again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "6.324555320336759\n",
      "[[0.]]\n",
      "[[6.]]\n"
     ]
    }
   ],
   "source": [
    "doc1_double = \"the quicker brown dogs easily jumps over the lazy dogs the quicker brown dogs easily jumps over the lazy dogs\" \n",
    "doc2_double = \"the quicker dogs pose a serious problem for lazy dogs the quicker dogs pose a serious problem for lazy dogs\"\n",
    "corpus = [doc1_double, doc2_double]\n",
    "\n",
    "tfs_doc1_double = calc_term_frequency(doc1_double)\n",
    "tfs_doc2_double = calc_term_frequency(doc2_double)\n",
    "\n",
    "print(math.sqrt(sum((tfs_doc1_double.get(k, 0) - tfs_doc1_double.get(k, 0))**2 for k in set(tfs_doc1_double.keys()).union(set(tfs_doc1_double.keys())))))\n",
    "print(math.sqrt(sum((tfs_doc1_double.get(k, 0) - tfs_doc2_double.get(k, 0))**2 for k in set(tfs_doc1_double.keys()).union(set(tfs_doc2_double.keys())))))\n",
    "\n",
    "corpus_vect = CountVectorizer().fit_transform(corpus).todense() \n",
    "\n",
    "print(euclidean_distances(corpus_vect[0], corpus_vect[0])) # output: 0\n",
    "print(euclidean_distances(corpus_vect[0], corpus_vect[1] )) # output: 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise surprise! The euclidean distance doubles!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.What is the cosine similarity between A' and B' (using raw term frequency)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short answer:** \n",
    "It doubles just like ED. \n",
    "\n",
    "**Long answer:**\n",
    "See below to subquestion C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for cosine similarity between 2 data objects is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\cos ({\\bf t},{\\bf e})= {{\\bf t} {\\bf e} \\over \\|{\\bf t}\\| \\|{\\bf e}\\|} = \\frac{ \\sum_{i=1}^{n}{{\\bf t}_i{\\bf e}_i} }{ \\sqrt{\\sum_{i=1}^{n}{({\\bf t}_i)^2}} \\sqrt{\\sum_{i=1}^{n}{({\\bf e}_i)^2}} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $||t||$ is the Euclidean norm of $t = (t_1, t_2, ..., t_n)$, defined as $\\sqrt{x_1^2 + x_2^2 + ... + x_p^2}$. Practically speaking, the len(t). $||e||$ = the Euclidean norm of vector e. So, the formula is vector t times vector e over the Euclidean norm of t times the Euclidean norm of e. A cosine value of 0 means that the two vectors are at 90 degrees to each other (orthogonal) and have no match. The closer the cosine value to 1, the smaller the angle and the greater the match between vectors. Let's run through an example, using the same docs as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"the quicker brown dogs easily jumps over the lazy dogs\" \n",
    "doc2 = \"the quicker dogs pose a serious problem for lazy dogs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sklearn.metrics.pairwise.cosine_similarity to double check that we get the right result, but let's compute it manually first. We can use the term frequency method above\n",
    "\n",
    "An alternative approach to what we did above, where we joined the sets of the 2 bags of words, is to simple find the intersection. The intuition here, is that if there is no intersection, then there is no similarity. Therfore, a measure of set intersection, $t \\cap e$, is equally valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6172133998483678"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cosine_similarity(vect_doc1, vect_doc2):\n",
    "    \n",
    "    union = set(tfs_doc1.keys()).union(set(tfs_doc1.keys()))\n",
    "    # intersection = set(vect_doc1.keys()) & set(vect_doc2.keys()) # getting intersection of set t and set e\n",
    "    \n",
    "    numerator = sum([vect_doc1.get(x, 0) * vect_doc2.get(x, 0) for x in union]) # define numerator\n",
    "\n",
    "    sum1 = sum([vect_doc1[x] ** 2 for x in list(vect_doc1.keys())]) # define sum for vectorized doc 1\n",
    "    sum2 = sum([vect_doc2[x] ** 2 for x in list(vect_doc2.keys())]) # define sum for vectorized doc 2\n",
    "    \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2) # define denominator\n",
    "\n",
    "\n",
    "    return float(numerator) / denominator\n",
    "\n",
    "\n",
    "cosine = get_cosine_similarity(tfs_doc1, tfs_doc2)\n",
    "\n",
    "cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we compare that to using sklearns CountVectorizer and cosine_similarity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.64465837],\n",
       "       [0.64465837, 1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "sparse_matrix = CountVectorizer().fit_transform([doc1, doc2])\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "cosine_similarity(sparse_matrix, sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our manually-computed cosine similarity is a little off. Perhaps a arithemtic mistake? For the purposes of this subquestion, it doesn't really matter, because what we interested in, is how the cosine similarity changes when the docs \"doube\" again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same things, but with the docs doubled once again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.64465837],\n",
       "       [0.64465837, 1.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    \"the quicker brown dogs easily jumps over the lazy dogs the quicker brown dogs easily jumps over the lazy dogs\",\n",
    "    \"the quicker dogs pose a serious problem for lazy dogs the quicker dogs pose a serious problem for lazy dogs\"\n",
    "]\n",
    "\n",
    "# Vectorise the data\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(data) # `X` will now be a TF representation of the data, the first row of `X` corresponds to the first sentence in `data`\n",
    "\n",
    "# Calculate the pairwise cosine similarities (depending on the amount of data that you are going to have this could take a while)\n",
    "S = cosine_similarity(X)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.What does this say about using cosine similarity as opposed to Euclidean distance in information retrieval?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This say that cosine similarity does not consider magnitude. The ratios between the docs have not changed, and so this is reflected in the score, eventhough the nominal counts have changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Suppose we run the SNOWBALL algorithm on the text below to attempt to extract the FOUNDER-OF relation. Which of the patterns below will extract at least one correct example of that relation without extracting any incorrect ones (select all that apply)?**\n",
    "\n",
    "a) ORG, founded by PERSON\n",
    "\n",
    "b) ORG, PERSON\n",
    "\n",
    "c) founders of ORG, PERSON\n",
    "\n",
    "d) PERSON of ORG\n",
    "\n",
    "ORG entities are bold, and PERSON entities are underlined. You can assume that all of the patterns are well formed SNOWBALL patterns. However, let's try and establish the entityis ourselves first.\n",
    "\n",
    "Correct examples: (Microsoft, Bill Gates) , (Facebook, Mark Zuckerberg) , (Google, Larry Page) , (Google, Sergey Brin)\n",
    "\n",
    "**Short answer:**\n",
    "Options A and C produces >= 1 results and no incorrect ones. \n",
    "\n",
    "\n",
    "**Long answer:** \n",
    "See below until question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = (\"Microsoft, founded by Bill Gates, produces both computer software and personal computers. \" +\n",
    "\"The founders of Google, Larry Page and Sergei Brin, developed an advanced search experience. \" + \n",
    "\"And Mark Zuckerberg, founder of Facebook, crafted a new communication platform. \" + \n",
    "\"And, usage exists between them: indeed, Bill Gates is a user of Google search, and Larry Page of Microsoft products \" +\n",
    "\"such as Word. Bill Gates of Microsoft, Larry Page and Sergei Brin of Google, and Mark Zuckerberg of Facebook were \" +\n",
    "\"all pioneers of today’s technology.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here is to extract FOUNDER-OF relations from the source text using the SNOWBALL algorithm. The algorithm itself is a follows:\n",
    "\n",
    "1. Start with a set of seed tuples (or extract a seed set from the unlabeled text with a few hand-crafted rules) - We have this already\n",
    "\n",
    "2. Extract occurrences from the unlabeled text that matches the tuples and tag them with a NER (named entity recognizer). - We can use NLTK or SpaCy\n",
    "\n",
    "3. Create patterns for these occurrences, e.g. “ORG is based in LOC”. - These are given.\n",
    "\n",
    "4. Generate new tuples from the text, e.g. (ORG:Intel, LOC: Santa Clara), and add to the seed set - No new sets exist.\n",
    "\n",
    "5. Go step 2 or terminate and use the patterns that were created for further extraction\n",
    "\n",
    "Snowball is a small string processing language designed for creating stemming algorithms for use in Information Retrieval. Ihe original paper is \"Snowball: Extracting Relations from Large Plain-Text Collections\", Agichtein & Gravano, 2000).\n",
    "\n",
    "Let's try out the different patterns. First we need to preproces the text, meaning that we need to do tokenization and POS tagging. For this, we could use SpaCy but since that library doesn't have a built-in relation extraction function, let's go with NLTK instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Microsoft/NNP)\n",
      "  ,/,\n",
      "  founded/VBN\n",
      "  by/IN\n",
      "  (PERSON Bill/NNP Gates/NNP)\n",
      "  ,/,\n",
      "  produces/VBZ\n",
      "  both/DT\n",
      "  computer/NN\n",
      "  software/NN\n",
      "  and/CC\n",
      "  personal/JJ\n",
      "  computers/NNS\n",
      "  ./.\n",
      "  The/DT\n",
      "  founders/NNS\n",
      "  of/IN\n",
      "  (GPE Google/NNP)\n",
      "  ,/,\n",
      "  (PERSON Larry/NNP Page/NNP)\n",
      "  and/CC\n",
      "  (PERSON Sergei/NNP Brin/NNP)\n",
      "  ,/,\n",
      "  developed/VBD\n",
      "  an/DT\n",
      "  advanced/JJ\n",
      "  search/NN\n",
      "  experience/NN\n",
      "  ./.\n",
      "  And/CC\n",
      "  (PERSON Mark/NNP Zuckerberg/NNP)\n",
      "  ,/,\n",
      "  founder/NN\n",
      "  of/IN\n",
      "  (GPE Facebook/NNP)\n",
      "  ,/,\n",
      "  crafted/VBD\n",
      "  a/DT\n",
      "  new/JJ\n",
      "  communication/NN\n",
      "  platform/NN\n",
      "  ./.\n",
      "  And/CC\n",
      "  ,/,\n",
      "  usage/JJ\n",
      "  exists/NNS\n",
      "  between/IN\n",
      "  them/PRP\n",
      "  :/:\n",
      "  indeed/RB\n",
      "  ,/,\n",
      "  (PERSON Bill/NNP Gates/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  user/NN\n",
      "  of/IN\n",
      "  (GPE Google/NNP)\n",
      "  search/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (PERSON Larry/NNP Page/NNP)\n",
      "  of/IN\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  products/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  (ORGANIZATION Word/NNP)\n",
      "  ./.\n",
      "  (PERSON Bill/NNP Gates/NNP)\n",
      "  of/IN\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  ,/,\n",
      "  (PERSON Larry/NNP Page/NNP)\n",
      "  and/CC\n",
      "  (PERSON Sergei/NNP Brin/NNP)\n",
      "  of/IN\n",
      "  (GPE Google/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  (PERSON Mark/NNP Zuckerberg/NNP)\n",
      "  of/IN\n",
      "  (ORGANIZATION Facebook/NNP)\n",
      "  were/VBD\n",
      "  all/DT\n",
      "  pioneers/NNS\n",
      "  of/IN\n",
      "  today/NN\n",
      "  ’/NNP\n",
      "  s/NN\n",
      "  technology/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "from nltk import Tree\n",
    "\n",
    "pos_tags = nltk.pos_tag(word_tokenize(src_text))  # POS tagging of the sentence\n",
    "ne = nltk.ne_chunk(pos_tags)  # Named Entity Recognition\n",
    "\n",
    "# ne.draw()\n",
    "print(ne) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue arises when using the nltk.ne_chunk_sents() method. In our text, some of the companies are classified as being of type \"GPE\"! This should be changable manually but goes beyond the scope of this assignment. Hours were wasted. Below are the patterns used to return the relation tuples, answering question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern 4: ('sergei_brin', 'google')\n",
      "Pattern 4: ('larry_page', 'microsoft')\n",
      "Pattern 4: ('bill_gates', 'microsoft')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pattern 3: r'????'\n",
    "# (c): correctly extracts (Google, Larry Page)\n",
    "\n",
    "# Pattern 4: r'\\bof\\b' needs reverse entity positions (PERSON, ORG) instead, and produces\n",
    "# ('larry_page', 'microsoft'); ('bill_gates', 'microsoft'); ('sergei_brin', 'google') which is wrong\n",
    "# (d): correctly  extracts  (Microsoft,  Bill  Gates),  (Sergei  Brin,  Google),  (Mark  Zuckerberg,Facebook),\n",
    "# but also incorrectly extracts (Microsoft, Larry Page\n",
    "\n",
    "\n",
    "def test_patterns(ne, subj, obj, pattern, relsym):\n",
    "    for rel in nltk.sem.extract_rels(subj, obj, ne, corpus='ace', pattern=pattern):\n",
    "        # print(nltk.sem.rtuple(rel))\n",
    "        print(nltk.sem.clause(rel, relsym=relsym))\n",
    "\n",
    "# Pattern A, \"ORG, founded by PERSON\", returns ('microsoft', 'bill_gates'), which is correct\n",
    "#test_patterns(ne, \"GPE\", \"PERSON\", re.compile(r'.*,.*founded.+by.+'), \"Pattern 1: \")\n",
    "\n",
    "# Pattern B, \"ORG, PERSON\", returns ('google', 'larry_page'); ('microsoft', 'larry_page') which is incorrect\n",
    "#test_patterns(ne, \"GPE\", \"PERSON\", re.compile(r','), \"Pattern 2: \")\n",
    "#test_patterns(ne, \"ORGANIZATION\", \"PERSON\", re.compile(r','), \"Pattern 2: \")\n",
    "\n",
    "# Pattern C, \"founders of ORG, PERSON\", returns ('Google, Larry Page'), which is correct\n",
    "#test_patterns(ne, \"GPE\", \"PERSON\", re.compile(r'[^,]+'), \"Pattern 3: \")\n",
    "\n",
    "#Pattern D, \"PERSON of ORG\", returns (\"Sergei Brin\", \"Google\"); (\"Larry Page\", \"Microoft\"); (\"Bill Gates\", Microsoft\") which is incorrect.\n",
    "test_patterns(ne, \"PERSON\", \"GPE\", re.compile(r'\\bof\\b'), \"Pattern 4: \")\n",
    "test_patterns(ne, \"PERSON\", \"ORGANIZATION\", re.compile(r'\\bof\\b'), \"Pattern 4: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Suppose that we run 3 queries in a QA system for its evaluation. For the 1st query, the system returns 10 candidate answers and the 4th is the first correct answer. For the 2nd query, the system returns 5 candidate answers and the 2nd is the first correct answer. For the 3rd query, the system returns 3 candidate answers and none of them is the correct answer. What is the mean reciprocal rank (MRR)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean reciprocal rank is a statistical measurements of evaluation of any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. \n",
    "\n",
    "The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, ​1⁄2 for second place, ​1⁄3 for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{MRR} = \\frac 1 Q \\sum_{i=1}^{Q} \\frac 1 {Rank_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quick answer, is that Q1 gives rank $1/4 = 0,25$, Q2 gives rank $1/2 = 0,5$ and Q3 gives rank 0, since no correct answer. Hence, the MRR is $1/3 * (1/4+1/2+0) = 0.25$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/3 * (1/4+1/2+0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
