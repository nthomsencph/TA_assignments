{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is part 1 of TA MA  #3: Simple problems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. An IR system returns 10 relevant documents and 8 non-relevant documents. There are a total of 25 relevant documents in the collection. What is the precision of the system on this search, and what is its recall?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = true positives / (true positives + false positives)\n",
    "\n",
    "Recall = true positives / (true positives + false negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 / (10 + 8) # precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 / (10 + 15) # recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Draw the inverted index that would be built for the following document collection.** \n",
    "\n",
    "Doc 1: new home sales top forecasts\n",
    "\n",
    "Doc 2: home sales rise in july\n",
    "\n",
    "Doc 3: increase in home sales in july\n",
    "\n",
    "Doc 4: july new home sales rise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple inverted index can be built by collection documents (above), tokenizing each document, preprocess to normalized tokens (indexing terms), and finally indexing the documents in an inverted index, consisting of a dict and postings. Lets just build it before we draw it. According to Manning's Information Retrieval, inverted index structures are essentially without rivals as the most efficient structure for supporting ad hoc text search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "docs = [\"new home sales, top forecasts#\", # inserted some non-alpa-numeric entities demonstrate normalization.\n",
    "        \"home sales rise, in #july\",\n",
    "        \"increase in home, sales in july\",\n",
    "        \"july new home#:)/--'' sales, rise\"] # list of docs\n",
    "\n",
    "tokenized_docs = [word_tokenize(s) for s in docs]\n",
    "normalized_docs = [[word.lower() for word in s if word.isalpha()] for s in tokenized_docs] # we also redudantly lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inv_indx = defaultdict(list) # using a defaultdict provides a defaul value for a nonexistent key as to avoid KeyErrors\n",
    "for idx, text in enumerate(normalized_docs): # enumerating over the list of normalized docs and their indexes\n",
    "    for word in text: \n",
    "        inv_indx[word].append(idx) # appending the indexes to which every word belongs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'new': [0, 3],\n",
       "             'home': [0, 1, 2, 3],\n",
       "             'sales': [0, 1, 2, 3],\n",
       "             'top': [0],\n",
       "             'forecasts': [0],\n",
       "             'rise': [1, 3],\n",
       "             'in': [1, 2, 2],\n",
       "             'july': [1, 2, 3],\n",
       "             'increase': [2]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_indx # I hope that the output below counts as a drawing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Consider two documents A and B whose Euclidean distance is d and cosine similarity is c (using no normalization other than raw term frequencies). If we create a new document A' by appending A to itself and another document B' by appending B to itself, then:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**a) What is the Euclidean distance between A' and B' (using raw term frequency)?**\n",
    "\n",
    "\n",
    "\n",
    "**Short answer:**\n",
    "The Euclidean Distance between A' and B' is $d^2$, e.g, for the example below, 3.1622776601683795 ** 2 = 6.324555320336759‬\n",
    "\n",
    "**Long answer:** \n",
    "See below until subquestion b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equaton for euclidian distance between 2 data objects is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d\\left( p,q\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( q_{i}-p_{i}\\right)^2 } $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where n is the number of dimensions / attributes, $p_k$ and $q_k$ are, respectively, the $Kth$ attributes of data objects p and q. Which basically means that we compute the distance of the individual attributes, sqaure it and then sum it all, before we take the square root of the sum. Let's run through an example, using raw term frequency (as opposed to TF-IDF maybe) and 2 sentences of equal length, len = 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"the quicker brown dogs easily jumps over the lazy dogs\" \n",
    "doc2 = \"the quicker dogs pose a serious problem for lazy dogs\"\n",
    "corpus = [doc1, doc2]\n",
    "len(doc1.split()), len(doc1.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we calculate raw the term frequencies for each doc, using the equation\n",
    "\n",
    "TF = (Number of time the word occurs in the text) / (Total number of words in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 2, 'quicker': 1, 'brown': 1, 'dogs': 2, 'easily': 1, 'jumps': 1, 'over': 1, 'lazy': 1}\n",
      "{'the': 1, 'quicker': 1, 'dogs': 2, 'pose': 1, 'a': 1, 'serious': 1, 'problem': 1, 'for': 1, 'lazy': 1}\n"
     ]
    }
   ],
   "source": [
    "def calc_term_frequency(doc : str):\n",
    "    \n",
    "    dic = {}\n",
    "    for word in doc.split():\n",
    "        if word in dic:\n",
    "            dic[word] = dic[word] + 1\n",
    "        else:\n",
    "            dic[word] = 1\n",
    "    \n",
    "    for word, frequency in dic.items():\n",
    "       dic[word] = frequency\n",
    "    \n",
    "    return dic\n",
    "\n",
    "tfs_doc1 = calc_term_frequency(doc1)\n",
    "tfs_doc2 = calc_term_frequency(doc2)\n",
    "print(tfs_doc1)\n",
    "print(tfs_doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate their inter partes euclidian distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1622776601683795"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "math.sqrt(sum((tfs_doc1.get(k, 0) - tfs_doc1.get(k, 0))**2 for k in set(tfs_doc1.keys()).union(set(tfs_doc1.keys())))) # output: 0\n",
    "math.sqrt(sum((tfs_doc1.get(k, 0) - tfs_doc2.get(k, 0))**2 for k in set(tfs_doc1.keys()).union(set(tfs_doc2.keys())))) # output: 0.316227766016838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n",
      "[[3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "corpus_vect = CountVectorizer().fit_transform(corpus)\n",
    "\n",
    "print(euclidean_distances(corpus_vect[0], corpus_vect[0])) # output: 0\n",
    "print(euclidean_distances(corpus_vect[0], corpus_vect[1] )) # output: 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computed distances above is the same as the one we got manually, when we round it down. Which means our base euclidian distance, d = 0.316227766016838\n",
    "\n",
    "Now, let's answer the question, what happens if we append each doc to itself and then run the whole thing again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "6.324555320336759\n",
      "[[0.]]\n",
      "[[6.]]\n"
     ]
    }
   ],
   "source": [
    "doc1_double = \"the quicker brown dogs easily jumps over the lazy dogs the quicker brown dogs easily jumps over the lazy dogs\" \n",
    "doc2_double = \"the quicker dogs pose a serious problem for lazy dogs the quicker dogs pose a serious problem for lazy dogs\"\n",
    "corpus = [doc1_double, doc2_double]\n",
    "\n",
    "tfs_doc1_double = calc_term_frequency(doc1_double)\n",
    "tfs_doc2_double = calc_term_frequency(doc2_double)\n",
    "\n",
    "print(math.sqrt(sum((tfs_doc1_double.get(k, 0) - tfs_doc1_double.get(k, 0))**2 for k in set(tfs_doc1_double.keys()).union(set(tfs_doc1_double.keys())))))\n",
    "print(math.sqrt(sum((tfs_doc1_double.get(k, 0) - tfs_doc2_double.get(k, 0))**2 for k in set(tfs_doc1_double.keys()).union(set(tfs_doc2_double.keys())))))\n",
    "\n",
    "corpus_vect = CountVectorizer().fit_transform(corpus).todense() \n",
    "\n",
    "print(euclidean_distances(corpus_vect[0], corpus_vect[0])) # output: 0\n",
    "print(euclidean_distances(corpus_vect[0], corpus_vect[1] )) # output: 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The euclidean distance doubles! As we double the string, frequency of all terms are duplicated. Before \"doubling\" to A' and B', we have a $(a_1, a_2, ..., a_d)$ and $(b_1, b_2, ..., b_d)$ frequency vector for document A and B, respectively. Here the Euclidian distance will be $sqrt((a_1-b_1)^2 + (a_2-b_2)^2 + ... + (a_d - b_d)^2)$. \n",
    "\n",
    "After doubling, we have \n",
    "\n",
    "$(2 * a_1, 2 * a_2, ..., 2 * a_d)$ and $(2 * b_1, 2 * b_2, ...,2 * b_d)$ and the distance is:\n",
    "\n",
    "$dist(A', B') = sqrt((2 * a_1- 2 * b_1)^2 + (2 * a_2 - 2 *b_2)^2 + ... + (2 * a_d - 2 * b_d)^2) = \n",
    "2 * sqrt((a_1-b_1)^2 + (a_2-b_2)^2 + ... + (a_d - b_d)^2) = 2 * dist(A,B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.What is the cosine similarity between A' and B' (using raw term frequency)?**\n",
    "\n",
    "The equation for cosine similarity between 2 data objects is\n",
    "\n",
    "$\\cos ({\\bf t},{\\bf e})= {{\\bf t} {\\bf e} \\over \\|{\\bf t}\\| \\|{\\bf e}\\|} = \\frac{ \\sum_{i=1}^{n}{{\\bf t}_i{\\bf e}_i} }{ \\sqrt{\\sum_{i=1}^{n}{({\\bf t}_i)^2}} \\sqrt{\\sum_{i=1}^{n}{({\\bf e}_i)^2}} }$\n",
    "\n",
    "where $||t||$ is the Euclidean norm of $t = (t_1, t_2, ..., t_n)$, defined as $\\sqrt{x_1^2 + x_2^2 + ... + x_p^2}$. Practically speaking, the len(t). $||e||$ = the Euclidean norm of vector e. So, the formula is vector t times vector e over the Euclidean norm of t times the Euclidean norm of e. A cosine value of 0 means that the two vectors are at 90 degrees to each other (orthogonal) and have no match. The closer the cosine value to 1, the smaller the angle and the greater the match between vectors. Let's run through an example, using the same docs as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short answer:** \n",
    "It remains the same, as opposed to ED which doubles.\n",
    "\n",
    "$\\cos ({\\bf A'},{\\bf B'}) = \\frac{( (2*a_1) * (2*b_1)  + (2*a_2) * (2*b_2) + .... + (2* a_d) * (2 * b_d) )} {\\sqrt{(2*a_1)^2  + (2 * a_2)^2  + ... + (2 * a_d)^2 }  \\sqrt{((2*b_1)^2  + (2 * b_2)^2  + ... + (2 * b_d)^2 }} = \\frac{ \\sum_{i=1}^{n}{{\\bf (2a_i)}{\\bf (2b_i)}} }{ \\sqrt{\\sum_{i=1}^{n}{({\\bf 2a}_i)^2}} \\sqrt{\\sum_{i=1}^{n}{({\\bf 2b}_i)^2}} }$\n",
    "\n",
    "**Long answer:**\n",
    "See below to subquestion C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Calculate cosine similarity for regular strings**\n",
    "\n",
    "Suppose we have two sentences, \n",
    "\n",
    "string_A = \"This is a test string to prove that our math is correct\", and \n",
    "\n",
    "string_B = \"This is also a string to prove that our math is correct\". \n",
    "\n",
    "When we compute the raw frequency, as above, we get 2 dicts, containing the words and the term frequency vectors:\n",
    "\n",
    "A = {'This': 1, 'is': 2, 'a': 1, 'test': 1, 'string': 1, 'to': 1, 'prove': 1, 'that': 1, 'our': 1, 'math': 1, 'correct': 1}, and \n",
    "\n",
    "B = {'This': 1, 'is': 2, 'also': 1, 'a': 1, 'string': 1, 'to': 1, 'prove': 1, 'that': 1, 'our': 1, 'math': 1, 'correct': 1}\n",
    " \n",
    "***\n",
    "\n",
    "From here we need to find the union (or the intersection!) of the 2 term frequency vectors, show below\n",
    "\n",
    "A union B = {'also', 'our', 'is', 'prove', 'correct', 'a', 'test', 'math', 'string', 'to', 'This', 'that'}\n",
    "\n",
    "***\n",
    "\n",
    "In the cos sim equation above, our numerator is the sum of the term frequency vector element corresponding to each value in each of the 2 dicts. Like so,\n",
    "$numerator = \\sum_{i=1}^{n}{{\\bf t}_i{\\bf e}_i} = (0 * 1 + 1 * 1 + 2 * 2 + 1 * 1 + 1 * 1 + 1 * 1 + 1 * 0 + 1 * 1 + 1 * 1 + 1 * 1 + 1 * 1 + 1 * 1) = 13$\n",
    "\n",
    ".\n",
    "***\n",
    "\n",
    "Likewise the denominator is sqrt(sum of all values in A squared) times sqrt(sum of all values in B squared), like so, \n",
    "$\\sqrt{1^2 + 2^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{14}$\n",
    "\n",
    "$\\sqrt{1^2 + 2^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{14}$\n",
    "\n",
    "$denominator = \\sqrt{14} \\sqrt{14} = 14$\n",
    "\n",
    "Which means that our cosine similarity between these 2 strings is $13/14 = 0.9285714285714286$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing the cosine similarity for the doubled strings**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same strings, string_A and string_B, we get a raw term frequency dict of\n",
    "\n",
    "A = {'This': 2, 'is': 4, 'a': 2, 'test': 2, 'string': 2, 'to': 2, 'prove': 2, 'that': 2, 'our': 2, 'math': 2, 'correct': 2}\n",
    "\n",
    "B = {'This': 2, 'is': 4, 'also': 2, 'a': 2, 'string': 2, 'to': 2, 'prove': 2, 'that': 2, 'our': 2, 'math': 2, 'correct': 2}\n",
    "\n",
    "***\n",
    "\n",
    "But our union (or intersect) still remain the same\n",
    "\n",
    "A union B = {'also', 'our', 'is', 'prove', 'correct', 'a', 'test', 'math', 'string', 'to', 'This', 'that'}\n",
    "\n",
    "So when we calculate the numerator\n",
    "\n",
    "$numerator = \\sum_{i=1}^{n}{{\\bf t}_i{\\bf e}_i} = (0 * 2 + 2 * 2 + 4 * 4 + 2 * 2 + 2 * 2 + 2 * 2 + 2 * 0 + 2 * 2 + 2 * 2 + 2 * 2 + 2 * 2 + 2 * 2) = 52$\n",
    "\n",
    "and the denominator\n",
    "\n",
    "$\\sqrt{2^2 + 4^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2} = \\sqrt{56}$\n",
    "\n",
    "$\\sqrt{2^2 + 4^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2 + 2^2} = \\sqrt{56}$\n",
    "\n",
    "$denominator = \\frac{\\sqrt{56}} {\\sqrt{56}} = 56$\n",
    "\n",
    "***\n",
    "\n",
    "Which means that our cosine similarity between these 2 strings is $52/56 = 0.9285714285714286$ \n",
    "\n",
    "***\n",
    "\n",
    "The equation is implemented below\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(vect_doc1, vect_doc2):\n",
    "    \n",
    "    union = set(tfs_doc1.keys()).union(set(tfs_doc2.keys()))\n",
    "    # intersection = set(vect_doc1.keys()) & set(vect_doc2.keys()) # getting intersection of set t and set e\n",
    "    \n",
    "    numerator = sum([vect_doc1.get(x, 0) * vect_doc2.get(x, 0) for x in union]) # define numerator\n",
    "\n",
    "    sum1 = sum([vect_doc1[x] ** 2 for x in list(vect_doc1.keys())]) # define sum for vectorized doc 1\n",
    "    sum2 = sum([vect_doc2[x] ** 2 for x in list(vect_doc2.keys())]) # define sum for vectorized doc 2\n",
    "    \n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2) # define denominator\n",
    "\n",
    "\n",
    "    return float(numerator) / denominator\n",
    "\n",
    "\n",
    "cosine = get_cosine_similarity(tfs_doc1, tfs_doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check, 2 identical strings would output 1, see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs_doc1 = calc_term_frequency(\"This should work\")\n",
    "tfs_doc2 = calc_term_frequency(\"This should work\")\n",
    "\n",
    "get_cosine_similarity(tfs_doc1, tfs_doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sklearn.metrics.pairwise.cosine_similarity to double check that we get the right result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.92307692],\n",
       "       [0.92307692, 1.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "string_A = \"This is a test string to prove that our math is correct\"\n",
    "string_B = \"This is also a string to prove that our math is correct\" \n",
    "\n",
    "# Create the Document Term Matrix\n",
    "sparse_matrix = CountVectorizer().fit_transform([string_A, string_B])\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "cosine_similarity(sparse_matrix, sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our manually-computed cosine similarity is a little off. Perhaps a arithemtic mistake? Or maybe SKlearn factors in more measures than I know of. For the purposes of this subquestion, it doesn't really matter, because what we interested in, is how the cosine similarity changes when the docs \"doube\" again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same things, but with the docs doubled once again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.92307692],\n",
       "       [0.92307692, 1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_A = \"This is a test string to prove that our math is correct This is a test string to prove that our math is correct\"\n",
    "string_B = \"This is also a string to prove that our math is correct This is also a string to prove that our math is correct\" \n",
    "\n",
    "# Vectorise the data\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform([string_A, string_B]) # `X` will now be a TF representation of the data, the first row of `X` corresponds to the first sentence in `data`\n",
    "\n",
    "# Calculate the pairwise cosine similarities (depending on the amount of data that you are going to have this could take a while)\n",
    "S = cosine_similarity(X)\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we've shown how to calculate both Euclidean distance and cosine similarity, as well as demonstrated what happens when the input is doubled. For ED it doubles and for CS it stays the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.What does this say about using cosine similarity as opposed to Euclidean distance in information retrieval?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This say that cosine similarity does not consider magnitude. Euclidean distance measures the nominal distance between 2 points. When the magnitude changes, the nominal distance changes too. However, cosine similarity measures the degrees between the 2 points from origo. As the magnitude changes, the degrees between these points stay the same. \n",
    "\n",
    "![\"difference\"](edvscs.png \"Difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Suppose we run the SNOWBALL algorithm on the text below to attempt to extract the FOUNDER-OF relation. Which of the patterns below will extract at least one correct example of that relation without extracting any incorrect ones (select all that apply)?**\n",
    "\n",
    "a) ORG, founded by PERSON\n",
    "\n",
    "b) ORG, PERSON\n",
    "\n",
    "c) founders of ORG, PERSON\n",
    "\n",
    "d) PERSON of ORG\n",
    "\n",
    "ORG entities are bold, and PERSON entities are underlined. You can assume that all of the patterns are well formed SNOWBALL patterns. However, let's try and establish the entityis ourselves first.\n",
    "\n",
    "Correct examples: (Microsoft, Bill Gates) , (Facebook, Mark Zuckerberg) , (Google, Larry Page) , (Google, Sergey Brin)\n",
    "\n",
    "**Short answer:**\n",
    "Options A and C produces >= 1 results and no incorrect ones. \n",
    "\n",
    "\n",
    "**Long answer:** \n",
    "See below until question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = (\"Microsoft, founded by Bill Gates, produces both computer software and personal computers. \" +\n",
    "\"The founders of Google, Larry Page and Sergei Brin, developed an advanced search experience. \" + \n",
    "\"And Mark Zuckerberg, founder of Facebook, crafted a new communication platform. \" + \n",
    "\"And, usage exists between them: indeed, Bill Gates is a user of Google search, and Larry Page of Microsoft products \" +\n",
    "\"such as Word. Bill Gates of Microsoft, Larry Page and Sergei Brin of Google, and Mark Zuckerberg of Facebook were \" +\n",
    "\"all pioneers of today’s technology.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here is to extract FOUNDER-OF relations from the source text using the SNOWBALL algorithm. The algorithm itself is a follows:\n",
    "\n",
    "1. Start with a set of seed tuples (or extract a seed set from the unlabeled text with a few hand-crafted rules) - We have this already\n",
    "\n",
    "2. Extract occurrences from the unlabeled text that matches the tuples and tag them with a NER (named entity recognizer). - We use NLTK\n",
    "\n",
    "3. Create patterns for these occurrences, e.g. “ORG is based in LOC”. - These are given.\n",
    "\n",
    "4. Generate new tuples from the text, e.g. (ORG:Intel, LOC: Santa Clara), and add to the seed set - No new sets exist.\n",
    "\n",
    "5. Go step 2 or terminate and use the patterns that were created for further extraction\n",
    "\n",
    "**Original paper** is \"Snowball: Extracting Relations from Large Plain-Text Collections\", Agichtein & Gravano, 2000). \n",
    "\n",
    "In the original algo, Snowball is initially given a handful of example tuples. For every such organization-location tuple $< o,l >$, Snowball finds segments of text in the document collection where $o$ and $l$ occur close to each other, just as DIPRE does, and analyzes the text that “connects” $o$ and $l$ to generate patterns. A key improvement of Snowball from the basic DIPRE method is that Snowball’s patterns include named-entity tags. An example of such a pattern is <LOCATION>-based <ORGANIZATION>. This pattern will not match any pair of strings connected by “-based.” Instead, <LOCATION> will only match a string identified by a tagger as an entity of type LOCATION. Similarly, <ORGANIZATION> will only match a string identified by a tagger as an entity of type ORGANIZATION. Figure 3 shows additional patterns that Snowball might generate, with named-entity tags.\n",
    "    \n",
    "ORGANIZATION’s headquarters in LOCATION\n",
    "    \n",
    "LOCATION-based ORGANIZATION\n",
    "\n",
    "ORGANIZATION, LOCATION\n",
    "\n",
    "Let's try out the different patterns. First we need to preproces the text, meaning that we need to do tokenization and POS tagging. For this, we could use SpaCy but since that library doesn't have a built-in relation extraction function, let's go with NLTK instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "ne = nltk.ne_chunk(pos_tag(word_tokenize(src_text)))  # Tokenization, POS tagging and Named Entity chunking\n",
    "\n",
    "# print([t.leaves() for t in ne.subtrees(lambda s: s.label() == \"PERSON\")]) # Get all leaves where label == person\n",
    "#print([t.leaves() for t in ne.subtrees(lambda s: s.label() == \"GPE\" or s.label() == \"ORGANIZATION\")]) # same for organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue arises when using the nltk.ne_chunk_sents() method. In our text, some of the companies are classified as being of type \"GPE\"! This should be changable manually but goes beyond the scope of this assignment. Hours were wasted. Below we clean up the prepped texts a bit to make the patterns easier. This is of course not essential, but it makes the patterns cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, element in enumerate(ne):\n",
    "    if \"NNP\" in str(ne[index]):\n",
    "        continue\n",
    "    else:\n",
    "        ne[index] = element[0]\n",
    "        #print(ne[index])\n",
    "\n",
    "# print(ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below is used to test pattern C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_pattern(pattern):\n",
    "\n",
    "    context = re.compile(pattern).findall(' '.join(str(ne).split()))\n",
    "    tuples = re.compile(r'\\(.*\\)').findall(str(context).replace(\"/NNP\", \"\"))\n",
    "    tuples = re.sub(r\"[\\[\\'()\\]]\", \"\", str(tuples))\n",
    "    return re.sub(r\"[A-Z]+\\s\", \"\", tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are tests of all four cases. A and C are correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Microsoft, Bill Gates) -> Pattern: \", founded by\"\n",
      "(Microsoft, Bill Gates) -> Pattern: \",\"\n",
      "(Google, Larry Page) -> Pattern: \",\"\n",
      "(Google, Mark Zuckerberg) -> Pattern: \",\"\n",
      "(Microsoft, Larry Page) -> Pattern: \",\"\n",
      "(Google , Larry Page) -> Pattern \"founders of ORG, PERSON\"\n",
      "(Sergei Brin, Google) -> Pattern: \"of\"\n",
      "(Larry Page, Microsoft) -> Pattern: \"of\"\n",
      "(Bill Gates, Microsoft) -> Pattern: \"of\"\n"
     ]
    }
   ],
   "source": [
    "def test_patterns(subj, obj, pattern):\n",
    "    for rel in nltk.sem.extract_rels(subj, obj, ne, corpus='ace', pattern=re.compile(pattern)):\n",
    "        print(f'({rel[\"subjtext\"]}, {rel[\"objtext\"]}) -> Pattern: \"{pattern}\"'.replace(\"/NNP\", \"\"))\n",
    "    \n",
    "# Pattern A, \"ORG, founded by PERSON\", returns (Microsoft, Bill Gates), which is correct\n",
    "test_patterns(\"GPE\", \"PERSON\", r', founded by')\n",
    "test_patterns(\"ORGANIZATION\", \"PERSON\", r', founded by')\n",
    "\n",
    "# Pattern B, \"ORG, PERSON\": yields (Google, Larry Page); (Microsoft, Bill Gates); (Google, Mark Zuckerberg); (Microsoft, Larry Page)\n",
    "test_patterns(\"GPE\", \"PERSON\", r',')\n",
    "test_patterns(\"ORGANIZATION\", \"PERSON\", r',')\n",
    "\n",
    "#Pattern C, \"founders of ORG, PERSON\", returns ('Google, Larry Page'), which is correct\n",
    "tuples_c = manual_pattern(r'founder(?:s)?\\sof\\s\\((?:ORGANIZATION|GPE)\\s\\w+\\/\\w+\\)\\s?,\\s\\(PERSON\\s\\w+\\/\\w+\\s\\w+\\/\\w+\\)')\n",
    "print(f'({tuples_c}) -> Pattern \"founders of ORG, PERSON\"')\n",
    "\n",
    "#Pattern D, \"PERSON of ORG\", returns (\"Sergei Brin\", \"Google\"); (\"Larry Page\", \"Microoft\"); (\"Bill Gates\", Microsoft\") which is incorrect.\n",
    "test_patterns(\"PERSON\", \"GPE\", r'of')\n",
    "test_patterns(\"PERSON\", \"ORGANIZATION\", r'of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Suppose that we run 3 queries in a QA system for its evaluation. For the 1st query, the system returns 10 candidate answers and the 4th is the first correct answer. For the 2nd query, the system returns 5 candidate answers and the 2nd is the first correct answer. For the 3rd query, the system returns 3 candidate answers and none of them is the correct answer. What is the mean reciprocal rank (MRR)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean reciprocal rank is a statistical measurements of evaluation of any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. \n",
    "\n",
    "The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, ​1⁄2 for second place, ​1⁄3 for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{MRR} = \\frac 1 Q \\sum_{i=1}^{Q} \\frac 1 {Rank_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quick answer, is that Q1 gives rank $1/4 = 0,25$, Q2 gives rank $1/2 = 0,5$ and Q3 gives rank 0, since no correct answer. Hence, the MRR is $1/3 * (1/4+1/2+0) = 0.25$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/3 * (1/4+1/2+0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
